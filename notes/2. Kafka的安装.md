## 安装
### 直接安装
在[官网](https://kafka.apache.org/quickstart)中可查阅相关文档.

* 下载安装包
  ```shell
    wget https://dlcdn.apache.org/kafka/3.4.0/kafka_2.13-3.4.0.tgz 
  ```
* 解压配置
  ```shell
      tar kafka_2.13-3.4.0.tgz
      cd kafka_2.13-3.4.0
  ```
* 启动
  启动kafka,本地环境必须安装Java8+.启动方式可以使用ZooKeeper或者KRAft启动.从Kafka2.8开始,KRAft就出现代替ZooKeeper,
  ,但是实际生产还不建议使用,仍然推荐使用ZooKeeper启动.
  ```shell
  # Start the Zookeeper service
  bin/zookeeper-server-start.sh config/zookeeper.properties
  # 另起一个终端,Start the Kafka broker service
  bin/kafka-server-start.sh config/server.properties
  ```

### Docker安装



## 创建一个主题存储事件
Kafka是一个分布式的事件流平台,允许跨及其读取,写入,存储和处理事件.而事件被组织和存储在主题中,所以在编写一个事件之前,
必须创建一个主题.主题类似于文件系统中的文件夹,事件是该文件夹中的文件.
```shell
bin/kafka-topics.sh --create --topic quickstart-events --bootstrap-server localhost:9092
```
> 运行kafka-topics.sh命令，可显示使用topic的额外参数.

## 将一些事件写入主题
Kafka的client通过Kafka的brokers,编写或阅读事件,brokers将以持久和容错的方式存储事件,甚至永久存储.
```shell
bin/kafka-console-producer.sh --topic quickstart-events --bootstrap-server localhost:9092
This is first event
This is second event
```
> 可以随时使用`Ctrl+c`停止生产者客户端

## 阅读事件
重新打开一个终端,并运行消费者客户端来读取刚刚创造的事件.
```shell
bin/kafka-console-consumer.sh --topic quickstart-events --from-beginning --bootstrap-server localhost:9092
This is my first event
This is my second event
```
> 可以随时使用`Ctrl+c`停止生产者客户端,由于事件被永久存储在卡夫卡，因此它们可以随心所欲地被尽可能多地阅读。您可以通过打开另一个终端会话并再次重新运行上一个命令来读取事件.

## 使用Kafka connect将数据导入/导出为事件流
可能在现有系统中有许多历史数据需要传输导Kafka.`Kafka Connect`可以连续将数据从外部系统摄取到Kafka.**它是一个运行连接器的可
扩展工具,它实现了与外部系统交互的自定义逻辑.因此,将现有系统与Kafka继承十分容易.**
* 首先需要确认将`connect-file-3.4.0.jar`添加到`Connect worker`配置中的`plugin.path`属性中.
  编辑`config/connect-standalone.properties`文件,添加`plugin.path`配置属性匹配以下内容
  ```shell
  > echo "plugin.path=libs/connect-file-3.4.0.jar"
  ```
* 创建一些种子数据测试
  ```shell
  > echo -e "foo\nbar" > test.txt
  ```
* 启动连接器
  接下来，启动两个连接器在独立模式下运行，这意味着它们在单个本地专用进程中运行。
  Kafka提供三个配置文件作为参数。第一个是Kafka Connect进程的配置，包含要连接的Kafka代理和数据的序列化格式等常见配置。
  其余的配置文件分别指定要创建的连接器。这些文件包括一个唯一的连接器名称、要实例化的连接器类以及连接器所需的任何其他配置。
  ```shell
  >     bin/connect-standalone.sh config/connect-standalone.properties config/connect-file-source.properties config/connect-file-sink.properties
  ```
  > 这些示例配置文件包含在Kafka中，使用您之前启动的默认本地集群配置并创建两个连接器：第一个是源连接器，从输入文件中读取行，并生成每个行到Kafka主题，第二个是接收器连接器，读取来自Kafka主题的消息，并在输出文件中生成每行。
* 查看操作日志
  一旦Kafka Connect过程开始，源连接器应开始从test.txt读取行，并将其生成到主题connect-test，Sink连接器应开始读取主题connect-test的消息，并将其写入文件test.sink.txt。我们可以通过检查输出文件的内容来验证数据是否已通过整个管道交付：
  ```shell
  > more test.sink.txt
  foo
  bar
  ```
  请注意，数据存储在Kafka主题connect-test，因此我们也可以运行控制台消费者以查看主题中的数据（或使用自定义消费者代码进行处理）：
  ```shell
  bin/kafka-console-consumer.sh --bootstrap-server localhost:9092 --topic connect-test --from-beginning{"schema":{"type":"string","optional":false},"payload":"foo"}{"schema":{"type":"string","optional":false},"payload":"bar"}...
  ```

## 使用kafka streams处理事件
一旦您的数据作为事件存储在Kafka中，您可以使用Java/Scala的Kafka Streams客户端库处理数据。它允许您实现关键任务实时应用程序和微服务，其中输入和/或输出数据存储在Kafka主题中。Kafka Streams将客户端编写和部署标准Java和Scala应用程序的简单性与Kafka服务器端集群技术的优势相结合，使这些应用程序具有高度可扩展性、弹性、容错性和分布性。该库支持一次处理、有状态操作和聚合、窗口、连接、基于事件时间的处理等。
```shell
KStream<String, String> textLines = builder.stream("quickstart-events");

        KTable<String, Long> wordCounts = textLines
        .flatMapValues(line -> Arrays.asList(line.toLowerCase().split(" ")))
        .groupBy((keyIgnored, word) -> word)
        .count();

        wordCounts.toStream().to("output-topic", Produced.with(Serdes.String(), Serdes.Long()));
```

## 终止kafka的环境
1. 使用`Ctrl+C`终止所有的生成者和消费者
2. 使用`Ctrl+C`终止kafka broker
3. 使用`Ctrl+C`终止ZooKeeper
4. 使用`Ctrl+C`删除本地Kafka相关数据
  ```shell
  $ rm -rf /tmp/kafka-logs /tmp/zookeeper /tmp/kraft-combined-logs
  ```